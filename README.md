**Chat with Local AI Models using Streamlit, Ollama, and LlamaIndex
**
This project is a real-time chatbot built with:

- Streamlit â€” for the interactive web UI
- Ollama â€” to run local LLMs like Llama3, Mistral, or Phi3
- LlamaIndex â€” to abstract prompts and enhance LLM interaction

100% local and offline â€” No API keys required. Your data stays on your machine.



â¸»

ğŸš€ Features
	â€¢	ğŸ” Real-time chat with response streaming
	â€¢	ğŸ§  Choose between multiple LLMs: Llama3, Phi3, Mistral7B
	â€¢	ğŸ› Sidebar model selector
	â€¢	ğŸ› Logging built-in for monitoring/debugging
	â€¢	ğŸ”Œ Easily extendable with document loaders, vector search, or RAG

â¸»

ğŸ“¸ Screenshot

(Optional: Replace with your real screenshot image)


â¸»

ğŸ“¦ Requirements
	â€¢	Python 3.9 or higher
	â€¢	Ollama installed and running (ollama serve)
	â€¢	Supported model pulled (e.g., ollama pull llama3)
	â€¢	macOS users: llama-index-llms-ollama is required

â¸»

ğŸ› ï¸ Installation & Setup

1. Clone the Repository

git clone https://github.com/your-username/llm-chat-app.git
cd llm-chat-app

2. Create a Virtual Environment

python3 -m venv venv_llm_chatapp
source venv_llm_chatapp/bin/activate

3. Install Python Dependencies

pip install -r requirements.txt

Or manually:

pip install streamlit llama-index llama-index-llms-ollama

4. Pull a Model with Ollama

Make sure Ollama is installed and running. Then:

ollama pull llama3  # or mistral, phi3, etc.



â¸»

â–¶ï¸ Running the App

streamlit run ollama-streamlit-app.py

Then open http://localhost:8501 in your browser.

â¸»

ğŸ’¬ Usage Guide
	1.	Choose a model from the sidebar (e.g., Llama3, Phi3, Mistral)
	2.	Type your question in the chat box
	3.	View the real-time streamed response from the selected model

â¸»

ğŸ§  How It Works
	â€¢	ollama-streamlit-app.py handles the full UI + backend
	â€¢	Uses llama_index.llms.ollama.Ollama for connecting to local LLMs
	â€¢	Chat history is tracked using st.session_state
	â€¢	Supports streaming responses and error logging

â¸»

ğŸ§ª Example Interaction
	1.	User selects Llama3
	2.	User asks: â€œWhat is the capital of France?â€
	3.	Response: â€œThe capital of France is Paris.â€

â¸»

ğŸ“ Project Structure

llm-chat-app/
â”œâ”€â”€ ollama-streamlit-app.py       # Main app file
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # You're here!
â”œâ”€â”€ chat-interface-app.png        # (Optional) UI screenshot
â””â”€â”€ venv_llm_chatapp/             # Your virtual environment (excluded in .gitignore)



â¸»

ğŸ¤ Contributing

Contributions are welcome!
Feel free to open issues, submit PRs, or suggest features.

â¸»

ğŸ“ƒ License

This project is open-source and available under the MIT License.

â¸»

ğŸ™ Acknowledgements
	â€¢	Streamlit
	â€¢	Ollama
	â€¢	LlamaIndex
